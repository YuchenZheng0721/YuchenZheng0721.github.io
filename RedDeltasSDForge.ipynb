{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db53571c3da2481c9b1b1f963fc7e9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcc4f901b0f34ebfab584d1322aa042b",
              "IPY_MODEL_0310fd0c569140f8b42990200dc291f4",
              "IPY_MODEL_6bad4d54a16c446280dc3047d6beadc0"
            ],
            "layout": "IPY_MODEL_644fff5ec7414d42b84106a3e4b4c358"
          }
        },
        "bcc4f901b0f34ebfab584d1322aa042b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4495e6ebb02a4caf8988f7dd379d8946",
            "placeholder": "​",
            "style": "IPY_MODEL_872961babb144048b4e19a5f50fb13ee",
            "value": "(…)rnautXL_version6Rundiffusion.safetensors: 100%"
          }
        },
        "0310fd0c569140f8b42990200dc291f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_389a473fc4ea4c4995a4b1d60b19ae1a",
            "max": 7105348560,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a1d63236d2b4e1ca67c09ab3e02ecbc",
            "value": 7105348560
          }
        },
        "6bad4d54a16c446280dc3047d6beadc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372e262ca86e4b76a4fd54ccffb6ea48",
            "placeholder": "​",
            "style": "IPY_MODEL_d91f8784bb224531891cfffffd067799",
            "value": " 7.11G/7.11G [02:10&lt;00:00, 61.9MB/s]"
          }
        },
        "644fff5ec7414d42b84106a3e4b4c358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4495e6ebb02a4caf8988f7dd379d8946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "872961babb144048b4e19a5f50fb13ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "389a473fc4ea4c4995a4b1d60b19ae1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a1d63236d2b4e1ca67c09ab3e02ecbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "372e262ca86e4b76a4fd54ccffb6ea48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d91f8784bb224531891cfffffd067799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuchenZheng0721/YuchenZheng0721.github.io/blob/master/RedDeltasSDForge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RedDelta's SD Forge Colab\n",
        "\n",
        "The notebook is intended to make it easy to use [Stable Diffusion WebUI Forge](https://github.com/lllyasviel/stable-diffusion-webui-forge) in Google Colab. If you run into any problems or have any suggestions, please create an Issue on Github.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "\n",
        "*   **USE_GOOGLE_DRIVE** - Stores the Forge installation in your GDrive, this makes it easy to store your generated images, checkpoints, installed extensions etc between sessions\n",
        "*   **UPDATE_FORGE** - Update your Forge installation when you run this notebook\n",
        "*   **INSTALL_DEPS** - Installs optional dependencies such as `insightface`\n",
        "*   **ALLOW_EXTENSION_INSTALLATION** - Allow installing extensions through the UI **Warning**: It is recommended to have **USE_USERNAME_AND_PASSWORD** turned on if you're using this, otherwise anyone with your Gradio URL could install malicious extensions on your instance\n",
        "*   **USE_USERNAME_AND_PASSWORD** - Require inputting a username and password when accessing the Gradio URL\n",
        "*   **USERNAME** - Username to use when accessing the Gradio URL\n",
        "*   **PASSWORD** - Password to use when accessing the Gradio URL\n",
        "\n"
      ],
      "metadata": {
        "id": "G1THoku412to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import safe_open\n",
        "import json\n",
        "\n",
        "# 替换为你的模型路径\n",
        "lora_path = \"/content/drive/MyDrive/stable-diffusion-webui-forge/models/Lora/corgy_pinkpig_webui.safetensors\"\n",
        "\n",
        "with safe_open(lora_path, framework=\"pt\") as f:\n",
        "    metadata = f.metadata()\n",
        "\n",
        "print(\"📌 Metadata:\")\n",
        "print(json.dumps(metadata, indent=2))\n",
        "\n"
      ],
      "metadata": {
        "id": "nAkoyciyn73V",
        "outputId": "6d34341c-ca6c-4bd4-bf75-e71e8bd25fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Metadata:\n",
            "{\n",
            "  \"ss_network_weights\": \"corgy_pinkpig_webui.safetensors\",\n",
            "  \"ss_text_encoder_lr\": \"0\",\n",
            "  \"format\": \"pt\",\n",
            "  \"ss_version\": \"1.0.0\",\n",
            "  \"ss_sd_model_name\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
            "  \"ss_output_name\": \"corgy_pinkpig\",\n",
            "  \"ss_training\": \"DreamBooth-LoRA\",\n",
            "  \"ss_network_module\": \"networks.lora\",\n",
            "  \"ss_trigger_token\": \"ZYC\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file, save_file\n",
        "import torch\n",
        "\n",
        "# === ✅ 你的原始 LoRA 权重路径 ===\n",
        "input_path = \"/content/drive/MyDrive/stable-diffusion-webui-forge/models/Lora/pytorch_lora_weights.safetensors\"\n",
        "\n",
        "# === ✅ 输出文件名（推荐加 _webui）===\n",
        "output_path = \"/content/drive/MyDrive/stable-diffusion-webui-forge/models/Lora/pytorch_lora_weights_webui.safetensors\"\n",
        "\n",
        "# === ✅ 你实际使用的参数（按你训练的填写）===\n",
        "trigger_word = \"ZYC\"\n",
        "base_model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "train_method = \"DreamBooth-LoRA\"\n",
        "use_text_encoder = False  # 如果你没启用 text_encoder LoRA，保持 False\n",
        "\n",
        "# === ✅ 读取原始权重 ===\n",
        "state_dict = load_file(input_path)\n",
        "\n",
        "# === ✅ 构造 metadata（WebUI 用）===\n",
        "metadata = {\n",
        "    \"ss_version\": \"1.0.0\",\n",
        "    \"ss_training\": train_method,\n",
        "    \"ss_trigger_token\": trigger_word,\n",
        "    \"ss_output_name\": \"corgy_pinkpig\",\n",
        "    \"ss_sd_model_name\": base_model,\n",
        "    \"format\": \"pt\",\n",
        "    \"ss_network_module\": \"networks.lora\",\n",
        "    \"ss_network_weights\": \"corgy_pinkpig_webui.safetensors\",\n",
        "    \"ss_text_encoder_lr\": \"0\" if not use_text_encoder else \"1e-5\"\n",
        "}\n",
        "\n",
        "# === ✅ 保存为新权重 ===\n",
        "save_file(tensors=state_dict, filename=output_path, metadata=metadata)\n",
        "\n",
        "print(\"✅ 新模型已保存，可用于 WebUI：\", output_path)\n"
      ],
      "metadata": {
        "id": "_dF7dzr5o-HM",
        "outputId": "3bfe3d89-cac5-4e42-f544-468f0baae049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 新模型已保存，可用于 WebUI： /content/drive/MyDrive/stable-diffusion-webui-forge/models/Lora/pytorch_lora_weights_webui.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置主模型保存目录\n",
        "checkpoint_dir = \"/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion\"\n",
        "checkpoint_path = f\"{checkpoint_dir}/sdxl-base-1.0.safetensors\"\n",
        "\n",
        "# 创建目录（如果还没有）\n",
        "!mkdir -p \"$checkpoint_dir\"\n",
        "\n",
        "# 下载 SDXL 主模型（支持断点续传）\n",
        "!wget -c -O \"$checkpoint_path\" https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\n"
      ],
      "metadata": {
        "id": "Faq8_mreiFpx",
        "outputId": "334c3211-a6f1-4730-adeb-72428921bb68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-02 22:04:02--  https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.11, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/64bfcd5ff462a99a04fd1ec8/3d6f740fa52572e1071b8ecb7c5f8a8e2cbef596a51121102877bd9900078891?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250402%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250402T220402Z&X-Amz-Expires=3600&X-Amz-Signature=0f7bac385958382ed3bee8a82aacb5d1d7d9d1abcaf79099590d497627534d14&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sd_xl_base_1.0.safetensors%3B+filename%3D%22sd_xl_base_1.0.safetensors%22%3B&x-id=GetObject&Expires=1743635042&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYzNTA0Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGJmY2Q1ZmY0NjJhOTlhMDRmZDFlYzgvM2Q2Zjc0MGZhNTI1NzJlMTA3MWI4ZWNiN2M1ZjhhOGUyY2JlZjU5NmE1MTEyMTEwMjg3N2JkOTkwMDA3ODg5MSoifV19&Signature=Ab9MdWxVbl2%7E3wNzYc11Lxqye4MKZzO%7EZXH3aMMoerHW%7EQNEYiyTfuImnTFnk541tUG%7EE7OYdTI0BF6s68HpODqfrlL%7Ep810pAbr70DVx-Xph9QB5dI1Kd5ok%7EfnO1v%7EGhud8JoBDfD6lPxOrVv32HAw9hyZilTZDd%7EceDyf37LAMIZyaNMuRveYrd8AXutr3IYoAgyQLj9Y0GoHmmE8gemCGWSbQLLMr-Ag4zfp1dze7sj-0lO-D0cl45Y7rLgMuSfjkCPCasBmXwCDaJHBZJoNzLHHwItiPIVtlU4xqbf2rgseZmEPp6GdQYINy-8MR3qJk3jkPqForqcpv9iaiA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-04-02 22:04:02--  https://cas-bridge.xethub.hf.co/xet-bridge-us/64bfcd5ff462a99a04fd1ec8/3d6f740fa52572e1071b8ecb7c5f8a8e2cbef596a51121102877bd9900078891?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250402%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250402T220402Z&X-Amz-Expires=3600&X-Amz-Signature=0f7bac385958382ed3bee8a82aacb5d1d7d9d1abcaf79099590d497627534d14&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sd_xl_base_1.0.safetensors%3B+filename%3D%22sd_xl_base_1.0.safetensors%22%3B&x-id=GetObject&Expires=1743635042&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYzNTA0Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NGJmY2Q1ZmY0NjJhOTlhMDRmZDFlYzgvM2Q2Zjc0MGZhNTI1NzJlMTA3MWI4ZWNiN2M1ZjhhOGUyY2JlZjU5NmE1MTEyMTEwMjg3N2JkOTkwMDA3ODg5MSoifV19&Signature=Ab9MdWxVbl2%7E3wNzYc11Lxqye4MKZzO%7EZXH3aMMoerHW%7EQNEYiyTfuImnTFnk541tUG%7EE7OYdTI0BF6s68HpODqfrlL%7Ep810pAbr70DVx-Xph9QB5dI1Kd5ok%7EfnO1v%7EGhud8JoBDfD6lPxOrVv32HAw9hyZilTZDd%7EceDyf37LAMIZyaNMuRveYrd8AXutr3IYoAgyQLj9Y0GoHmmE8gemCGWSbQLLMr-Ag4zfp1dze7sj-0lO-D0cl45Y7rLgMuSfjkCPCasBmXwCDaJHBZJoNzLHHwItiPIVtlU4xqbf2rgseZmEPp6GdQYINy-8MR3qJk3jkPqForqcpv9iaiA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.238.29, 18.238.238.105, 18.238.238.109, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.238.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6938078334 (6.5G)\n",
            "Saving to: ‘/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/sdxl-base-1.0.safetensors’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   6.46G  51.1MB/s    in 2m 6s   \n",
            "\n",
            "2025-04-02 22:06:08 (52.6 MB/s) - ‘/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/sdxl-base-1.0.safetensors’ saved [6938078334/6938078334]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Hugging Face 模型信息\n",
        "repo_id = \"RunDiffusion/Juggernaut-XL-v6\"\n",
        "filename = \"juggernautXL_version6Rundiffusion.safetensors\"\n",
        "\n",
        "# 2. WebUI Forge 目标路径（修改成你的路径）\n",
        "target_dir = \"/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion\"\n",
        "\n",
        "# 创建目标文件夹（如果不存在）\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# 3. 下载模型\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=repo_id,\n",
        "    filename=filename\n",
        ")\n",
        "\n",
        "# 4. 拷贝到 WebUI Forge 路径下\n",
        "shutil.copy(model_path, os.path.join(target_dir, filename))\n",
        "\n",
        "print(f\"✅ 模型已下载并拷贝至 WebUI Forge 模型目录：{os.path.join(target_dir, filename)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lFycYNXkNCOz",
        "outputId": "f09a5730-82f1-40ee-c788-fe084a27a667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "db53571c3da2481c9b1b1f963fc7e9a5",
            "bcc4f901b0f34ebfab584d1322aa042b",
            "0310fd0c569140f8b42990200dc291f4",
            "6bad4d54a16c446280dc3047d6beadc0",
            "644fff5ec7414d42b84106a3e4b4c358",
            "4495e6ebb02a4caf8988f7dd379d8946",
            "872961babb144048b4e19a5f50fb13ee",
            "389a473fc4ea4c4995a4b1d60b19ae1a",
            "4a1d63236d2b4e1ca67c09ab3e02ecbc",
            "372e262ca86e4b76a4fd54ccffb6ea48",
            "d91f8784bb224531891cfffffd067799"
          ]
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)rnautXL_version6Rundiffusion.safetensors:   0%|          | 0.00/7.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db53571c3da2481c9b1b1f963fc7e9a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 模型已下载并拷贝至 WebUI Forge 模型目录：/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/juggernautXL_version6Rundiffusion.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from safetensors.torch import load_file\n",
        "\n",
        "lora_path = \"models/Lora/pytorch_lora_weights_webui.safetensors\"\n",
        "weights = load_file(lora_path)\n",
        "for key in weights:\n",
        "    print(key)\n"
      ],
      "metadata": {
        "id": "3ImjAYefE9W4",
        "outputId": "5d9ed3b7-8934-4b70-c469-039889a6fd93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_encoder.text_model.encoder.layers.0.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.0.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.1.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.10.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.11.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.2.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.3.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.4.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.5.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.6.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.7.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.8.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder.text_model.encoder.layers.9.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.0.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.1.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.10.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.11.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.12.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.13.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.14.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.15.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.16.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.17.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.18.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.19.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.2.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.20.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.21.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.22.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.23.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.24.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.25.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.26.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.27.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.28.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.29.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.3.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.30.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.31.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.4.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.5.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.6.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.7.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.8.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.k_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.k_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.out_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.out_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.q_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.q_proj.lora_linear_layer.up.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.v_proj.lora_linear_layer.down.weight\n",
            "text_encoder_2.text_model.encoder.layers.9.self_attn.v_proj.lora_linear_layer.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.lora.up.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.lora.down.weight\n",
            "unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.lora.up.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.lora.down.weight\n",
            "unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.lora.up.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.lora.down.weight\n",
            "unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.lora.up.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_FORGE = True  #@param {type:\"boolean\"}\n",
        "INSTALL_DEPS = True #@param {type:\"boolean\"}\n",
        "ALLOW_EXTENSION_INSTALLATION = True #@param {type:\"boolean\"}\n",
        "USE_USERNAME_AND_PASSWORD = True #@param {type:\"boolean\"}\n",
        "USERNAME = \"RedDeltas\" # @param {type:\"string\"}\n",
        "PASSWORD = \"IsTheBest\" # @param {type:\"string\"}\n",
        "\n",
        "WORKSPACE = 'stable-diffusion-webui-forge'\n",
        "\n",
        "if USE_GOOGLE_DRIVE:\n",
        "  print(\"📂 Connecting to Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  WORKSPACE = \"/content/drive/MyDrive/stable-diffusion-webui-forge\"\n",
        "  %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup SDForge =- && git clone --config core.filemode=false https://github.com/lllyasviel/stable-diffusion-webui-forge.git\n",
        "%cd $WORKSPACE\n",
        "import os\n",
        "\n",
        "# ✅ 安装 LayerDiffuse 扩展（如果没安装过）\n",
        "EXTENSION_PATH = f\"{WORKSPACE}/extensions/sd-forge-layerdiffuse\"\n",
        "if not os.path.exists(EXTENSION_PATH):\n",
        "    print(\"📦 Installing LayerDiffuse extension...\")\n",
        "    !git clone https://github.com/lllyasviel/sd-forge-layerdiffuse \"$EXTENSION_PATH\"\n",
        "else:\n",
        "    print(\"✅ LayerDiffuse already installed.\")\n",
        "\n",
        "extra_args = []\n",
        "\n",
        "if UPDATE_FORGE:\n",
        "  !echo -= Updating SDForge =-\n",
        "  !git pull\n",
        "\n",
        "if INSTALL_DEPS:\n",
        "  !echo -= Install dependencies =-\n",
        "  !pip3 install insightface\n",
        "\n",
        "if ALLOW_EXTENSION_INSTALLATION:\n",
        "  extra_args.append(\"--enable-insecure-extension-access\")\n",
        "\n",
        "if USE_USERNAME_AND_PASSWORD:\n",
        "  extra_args.append(f\"--gradio-auth {USERNAME}:{PASSWORD}\")\n",
        "\n",
        "extra_args_concat = \" \".join(extra_args)\n",
        "\n",
        "!python launch.py --share {extra_args_concat}"
      ],
      "metadata": {
        "id": "rEeZ9in5tBhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d42bb1-ccc1-456a-abe0-e0882b2adc9b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Connecting to Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/stable-diffusion-webui-forge\n",
            "✅ LayerDiffuse already installed.\n",
            "-= Updating SDForge =-\n",
            "Already up to date.\n",
            "-= Install dependencies =-\n",
            "Requirement already satisfied: insightface in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from insightface) (1.26.2)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from insightface) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from insightface) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from insightface) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from insightface) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from insightface) (9.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from insightface) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from insightface) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from insightface) (0.21.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from insightface) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from insightface) (3.0.12)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (from insightface) (2.0.5)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from insightface) (3.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (6.0.2)\n",
            "Collecting pydantic>=2.9.2 (from albumentations->insightface)\n",
            "  Using cached pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations->insightface) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations->insightface) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations->insightface) (6.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->insightface) (2.8.2)\n",
            "Collecting protobuf>=3.20.2 (from onnx->insightface)\n",
            "  Using cached protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->insightface) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->insightface) (2025.1.31)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (2025.3.13)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (1.8.0)\n",
            "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image->insightface) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->insightface) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.7.0)\n",
            "Collecting pydantic-core==2.33.0 (from pydantic>=2.9.2->albumentations->insightface)\n",
            "  Using cached pydantic_core-2.33.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.17.0)\n",
            "Using cached protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "Using cached pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
            "Using cached pydantic_core-2.33.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: pydantic-core, protobuf, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.20.1\n",
            "    Uninstalling pydantic_core-2.20.1:\n",
            "      Successfully uninstalled pydantic_core-2.20.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.0\n",
            "    Uninstalling protobuf-3.20.0:\n",
            "      Successfully uninstalled protobuf-3.20.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "open-clip-torch 2.20.0 requires protobuf<4, but you have protobuf 6.30.2 which is incompatible.\n",
            "mediapipe 0.10.21 requires protobuf<5,>=4.25.3, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-cloud-firestore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "wandb 0.19.8 requires protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 6.30.2 which is incompatible.\n",
            "google-cloud-aiplatform 1.86.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-genai 1.8.0 requires anyio<5.0.0,>=4.8.0, but you have anyio 3.7.1 which is incompatible.\n",
            "google-genai 1.8.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.24.1 which is incompatible.\n",
            "google-genai 1.8.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-6.30.2 pydantic-2.11.1 pydantic-core-2.33.0\n",
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "Version: f2.0.1v1.10.1-previous-659-gc055f2d4\n",
            "Commit hash: c055f2d43b07cbfd87ac3da4899a6d7ee52ebab9\n",
            "Installing requirements\n",
            "Launching Web UI with arguments: --share --enable-insecure-extension-access --gradio-auth RedDeltas:IsTheBest\n",
            "Total VRAM 15095 MB, total RAM 12979 MB\n",
            "pytorch version: 2.6.0+cu124\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype preferences: [torch.float32] -> torch.float32\n",
            "CUDA Using Stream: False\n",
            "2025-04-03 02:05:20.930490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743645920.960378   78065 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743645920.969401   78065 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-03 02:05:21.012072: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using pytorch cross attention\n",
            "Using pytorch attention for VAE\n",
            "ControlNet preprocessor location: /content/drive/MyDrive/stable-diffusion-webui-forge/models/ControlNetPreprocessor\n",
            "2025-04-03 02:05:39,385 - ControlNet - \u001b[0;32mINFO\u001b[0m - ControlNet UI callback registered.\n",
            "Checkpoint models--RunDiffusion--Juggernaut-XL/.no_exist/0ba557b600054da23791fe8f150237e354776036/juggernautXL_v8Rundiffusion.safetensors not found; loading fallback juggernautXL_version2.safetensors [700528894b]\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/juggernautXL_version2.safetensors', 'hash': '083b7e6b'}, 'additional_modules': ['/content/drive/MyDrive/stable-diffusion-webui-forge/models/VAE/sdxl.vae.safetensors'], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://cdf67ed3baf0d32a18.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Startup time: 72.0s (prepare environment: 35.6s, launcher: 1.0s, import torch: 24.3s, initialize shared: 0.2s, other imports: 0.8s, list SD models: 0.2s, load scripts: 4.2s, create ui: 3.6s, gradio launch: 1.9s).\n",
            "Environment vars changed: {'stream': False, 'inference_memory': 1024.0, 'pin_shared_memory': False}\n",
            "[GPU Setting] You will use 93.22% GPU memory (14071.00 MB) to load weights, and use 6.78% GPU memory (1024.00 MB) to do matrix computation.\n",
            "Model selected: {'checkpoint_info': {'filename': '/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/juggernautXL_version6Rundiffusion.safetensors', 'hash': '7d723a7b'}, 'additional_modules': ['/content/drive/MyDrive/stable-diffusion-webui-forge/models/VAE/sdxl.vae.safetensors'], 'unet_storage_dtype': None}\n",
            "Using online LoRAs in FP16: False\n",
            "Loading Model: {'checkpoint_info': {'filename': '/content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/juggernautXL_version6Rundiffusion.safetensors', 'hash': '7d723a7b'}, 'additional_modules': ['/content/drive/MyDrive/stable-diffusion-webui-forge/models/VAE/sdxl.vae.safetensors'], 'unet_storage_dtype': None}\n",
            "[Unload] Trying to free all memory for cuda:0 with 0 models keep loaded ... Done.\n",
            "StateDict Keys: {'unet': 1680, 'vae': 250, 'text_encoder': 197, 'text_encoder_2': 518, 'ignore': 0}\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "IntegratedAutoencoderKL Unexpected: ['model_ema.decay', 'model_ema.num_updates']\n",
            "K-Model Created: {'storage_dtype': torch.float16, 'computation_dtype': torch.float16}\n",
            "Calculating sha256 for /content/drive/MyDrive/stable-diffusion-webui-forge/models/Stable-diffusion/juggernautXL_version6Rundiffusion.safetensors: 1fe6c7ec54c786040cdabc7b4e89720069d97096922e20d01f13e7764412b47f\n",
            "Model loaded in 82.9s (unload existing model: 0.6s, forge model load: 30.9s, calculate hash: 51.5s).\n",
            "[Unload] Trying to free 3051.58 MB for cuda:0 with 0 models keep loaded ... Done.\n",
            "[Memory Management] Target: JointTextEncoder, Free GPU: 9972.72 MB, Model Require: 1559.68 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 7389.05 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 6.96 seconds\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 8173.20 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 5715.85 MB for cuda:0 with 0 models keep loaded ... Current free memory is 8172.40 MB ... Done.\n",
            "[Memory Management] Target: IntegratedAutoencoderKL, Free GPU: 8172.40 MB, Model Require: 319.11 MB, Previously Loaded: 0.00 MB, Inference Require: 1024.00 MB, Remaining: 6829.28 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.10 seconds\n",
            "[Unload] Trying to free 2615.54 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7849.15 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7849.15 MB, Model Require: 0.00 MB, Previously Loaded: 4897.05 MB, Inference Require: 1024.00 MB, Remaining: 6825.15 MB, All loaded to GPU.\n",
            "Merged with diffusion_model.input_blocks.0.0.weight channel changed to [320, 8, 3, 3]\n",
            "Moving model(s) has taken 4.35 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:01<00:30,  1.05s/it]\n",
            "  7% 2/30 [00:01<00:22,  1.24it/s]\n",
            " 10% 3/30 [00:02<00:19,  1.37it/s]\n",
            " 13% 4/30 [00:02<00:18,  1.44it/s]\n",
            " 17% 5/30 [00:03<00:16,  1.48it/s]\n",
            " 20% 6/30 [00:04<00:15,  1.51it/s]\n",
            " 23% 7/30 [00:04<00:15,  1.53it/s]\n",
            " 27% 8/30 [00:05<00:14,  1.54it/s]\n",
            " 30% 9/30 [00:06<00:13,  1.55it/s]\n",
            " 33% 10/30 [00:06<00:12,  1.55it/s]\n",
            " 37% 11/30 [00:07<00:12,  1.55it/s]\n",
            " 40% 12/30 [00:08<00:11,  1.55it/s]\n",
            " 43% 13/30 [00:08<00:11,  1.53it/s]\n",
            " 47% 14/30 [00:09<00:10,  1.54it/s]\n",
            " 50% 15/30 [00:10<00:09,  1.54it/s]\n",
            " 53% 16/30 [00:10<00:09,  1.53it/s]\n",
            " 57% 17/30 [00:11<00:08,  1.54it/s]\n",
            " 60% 18/30 [00:12<00:07,  1.53it/s]\n",
            " 63% 19/30 [00:12<00:07,  1.53it/s]\n",
            " 67% 20/30 [00:13<00:06,  1.53it/s]\n",
            " 70% 21/30 [00:13<00:05,  1.52it/s]\n",
            " 73% 22/30 [00:14<00:05,  1.52it/s]\n",
            " 77% 23/30 [00:15<00:04,  1.51it/s]\n",
            " 80% 24/30 [00:15<00:03,  1.50it/s]\n",
            " 83% 25/30 [00:16<00:03,  1.50it/s]\n",
            " 87% 26/30 [00:17<00:02,  1.50it/s]\n",
            " 90% 27/30 [00:17<00:02,  1.50it/s]\n",
            " 93% 28/30 [00:18<00:01,  1.50it/s]\n",
            " 97% 29/30 [00:19<00:00,  1.50it/s]\n",
            "100% 30/30 [00:20<00:00,  1.50it/s]\n",
            "[Unload] Trying to free 6534.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7835.86 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:21<00:00,  1.41it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7836.05 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7835.44 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 5301.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7834.64 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7834.64 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7834.64 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6810.64 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.04 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:18,  1.54it/s]\n",
            "  7% 2/30 [00:01<00:18,  1.48it/s]\n",
            " 10% 3/30 [00:02<00:18,  1.46it/s]\n",
            " 13% 4/30 [00:02<00:17,  1.47it/s]\n",
            " 17% 5/30 [00:03<00:17,  1.46it/s]\n",
            " 20% 6/30 [00:04<00:16,  1.46it/s]\n",
            " 23% 7/30 [00:04<00:15,  1.47it/s]\n",
            " 27% 8/30 [00:05<00:15,  1.47it/s]\n",
            " 30% 9/30 [00:06<00:14,  1.47it/s]\n",
            " 33% 10/30 [00:06<00:13,  1.47it/s]\n",
            " 37% 11/30 [00:07<00:12,  1.46it/s]\n",
            " 40% 12/30 [00:08<00:12,  1.46it/s]\n",
            " 43% 13/30 [00:08<00:11,  1.46it/s]\n",
            " 47% 14/30 [00:09<00:11,  1.45it/s]\n",
            " 50% 15/30 [00:10<00:10,  1.45it/s]\n",
            " 53% 16/30 [00:10<00:09,  1.45it/s]\n",
            " 57% 17/30 [00:11<00:09,  1.44it/s]\n",
            " 60% 18/30 [00:12<00:08,  1.44it/s]\n",
            " 63% 19/30 [00:13<00:07,  1.43it/s]\n",
            " 67% 20/30 [00:13<00:06,  1.43it/s]\n",
            " 70% 21/30 [00:14<00:06,  1.43it/s]\n",
            " 73% 22/30 [00:15<00:05,  1.43it/s]\n",
            " 77% 23/30 [00:15<00:04,  1.42it/s]\n",
            " 80% 24/30 [00:16<00:04,  1.41it/s]\n",
            " 83% 25/30 [00:17<00:03,  1.41it/s]\n",
            " 87% 26/30 [00:18<00:02,  1.41it/s]\n",
            " 90% 27/30 [00:18<00:02,  1.40it/s]\n",
            " 93% 28/30 [00:19<00:01,  1.40it/s]\n",
            " 97% 29/30 [00:20<00:00,  1.40it/s]\n",
            "100% 30/30 [00:20<00:00,  1.44it/s]\n",
            "[Unload] Trying to free 6534.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7834.45 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:22<00:00,  1.31it/s]\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 5301.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7834.45 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7834.45 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7834.45 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6810.45 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:21,  1.38it/s]\n",
            "  7% 2/30 [00:01<00:20,  1.36it/s]\n",
            " 10% 3/30 [00:02<00:19,  1.36it/s]\n",
            " 13% 4/30 [00:02<00:19,  1.36it/s]\n",
            " 17% 5/30 [00:03<00:18,  1.36it/s]\n",
            " 20% 6/30 [00:04<00:17,  1.35it/s]\n",
            " 23% 7/30 [00:05<00:17,  1.35it/s]\n",
            " 27% 8/30 [00:05<00:16,  1.35it/s]\n",
            " 30% 9/30 [00:06<00:15,  1.35it/s]\n",
            " 33% 10/30 [00:07<00:14,  1.34it/s]\n",
            " 37% 11/30 [00:08<00:14,  1.34it/s]\n",
            " 40% 12/30 [00:08<00:13,  1.34it/s]\n",
            " 43% 13/30 [00:09<00:12,  1.33it/s]\n",
            " 47% 14/30 [00:10<00:11,  1.34it/s]\n",
            " 50% 15/30 [00:11<00:11,  1.34it/s]\n",
            " 53% 16/30 [00:11<00:10,  1.34it/s]\n",
            " 57% 17/30 [00:12<00:09,  1.34it/s]\n",
            " 60% 18/30 [00:13<00:08,  1.34it/s]\n",
            " 63% 19/30 [00:14<00:08,  1.34it/s]\n",
            " 67% 20/30 [00:14<00:07,  1.34it/s]\n",
            " 70% 21/30 [00:15<00:06,  1.32it/s]\n",
            " 73% 22/30 [00:16<00:06,  1.33it/s]\n",
            " 77% 23/30 [00:17<00:05,  1.34it/s]\n",
            " 80% 24/30 [00:17<00:04,  1.34it/s]\n",
            " 83% 25/30 [00:18<00:03,  1.35it/s]\n",
            " 87% 26/30 [00:19<00:02,  1.35it/s]\n",
            " 90% 27/30 [00:20<00:02,  1.35it/s]\n",
            " 93% 28/30 [00:20<00:01,  1.35it/s]\n",
            " 97% 29/30 [00:21<00:00,  1.36it/s]\n",
            "100% 30/30 [00:22<00:00,  1.35it/s]\n",
            "[Unload] Trying to free 6534.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7823.45 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:24<00:00,  1.24it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7823.64 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7823.02 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 5301.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7822.22 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7822.22 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7822.22 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6798.22 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.05 seconds\n",
            "  0% 0/20 [00:00<?, ?it/s]\n",
            "  5% 1/20 [00:00<00:12,  1.55it/s]\n",
            " 10% 2/20 [00:01<00:12,  1.49it/s]\n",
            " 15% 3/20 [00:02<00:11,  1.49it/s]\n",
            " 20% 4/20 [00:02<00:10,  1.50it/s]\n",
            " 25% 5/20 [00:03<00:10,  1.48it/s]\n",
            " 30% 6/20 [00:04<00:09,  1.48it/s]\n",
            " 35% 7/20 [00:04<00:08,  1.48it/s]\n",
            " 40% 8/20 [00:05<00:08,  1.47it/s]\n",
            " 45% 9/20 [00:06<00:07,  1.47it/s]\n",
            " 50% 10/20 [00:06<00:06,  1.47it/s]\n",
            " 55% 11/20 [00:07<00:06,  1.46it/s]\n",
            " 60% 12/20 [00:08<00:05,  1.46it/s]\n",
            " 65% 13/20 [00:08<00:04,  1.46it/s]\n",
            " 70% 14/20 [00:09<00:04,  1.45it/s]\n",
            " 75% 15/20 [00:10<00:03,  1.44it/s]\n",
            " 80% 16/20 [00:10<00:02,  1.44it/s]\n",
            " 85% 17/20 [00:11<00:02,  1.44it/s]\n",
            " 90% 18/20 [00:12<00:01,  1.43it/s]\n",
            " 95% 19/20 [00:13<00:00,  1.43it/s]\n",
            "100% 20/20 [00:13<00:00,  1.46it/s]\n",
            "[Unload] Trying to free 6534.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7822.22 MB ... Done.\n",
            "\n",
            "Total progress: 100% 20/20 [00:15<00:00,  1.28it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7822.41 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7821.80 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7821.07 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7821.07 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7821.07 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6797.07 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/20 [00:00<?, ?it/s]\n",
            "  5% 1/20 [00:00<00:08,  2.21it/s]\n",
            " 10% 2/20 [00:00<00:08,  2.24it/s]\n",
            " 15% 3/20 [00:01<00:07,  2.23it/s]\n",
            " 20% 4/20 [00:01<00:07,  2.24it/s]\n",
            " 25% 5/20 [00:02<00:06,  2.22it/s]\n",
            " 30% 6/20 [00:02<00:06,  2.23it/s]\n",
            " 35% 7/20 [00:03<00:05,  2.24it/s]\n",
            " 40% 8/20 [00:03<00:05,  2.23it/s]\n",
            " 45% 9/20 [00:04<00:04,  2.21it/s]\n",
            " 50% 10/20 [00:04<00:04,  2.21it/s]\n",
            " 55% 11/20 [00:04<00:04,  2.20it/s]\n",
            " 60% 12/20 [00:05<00:03,  2.21it/s]\n",
            " 65% 13/20 [00:05<00:03,  2.20it/s]\n",
            " 70% 14/20 [00:06<00:02,  2.19it/s]\n",
            " 75% 15/20 [00:06<00:02,  2.19it/s]\n",
            " 80% 16/20 [00:07<00:01,  2.18it/s]\n",
            " 85% 17/20 [00:07<00:01,  2.19it/s]\n",
            " 90% 18/20 [00:08<00:00,  2.17it/s]\n",
            " 95% 19/20 [00:08<00:00,  2.17it/s]\n",
            "100% 20/20 [00:09<00:00,  2.20it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7815.49 MB ... Done.\n",
            "\n",
            "Total progress: 100% 20/20 [00:10<00:00,  1.96it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7815.61 MB ... Done.\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7815.00 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7814.27 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7814.27 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7814.27 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6790.27 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:13,  2.21it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.26it/s]\n",
            " 10% 3/30 [00:01<00:12,  2.24it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.24it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.24it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.24it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.24it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.23it/s]\n",
            " 30% 9/30 [00:04<00:09,  2.23it/s]\n",
            " 33% 10/30 [00:04<00:08,  2.22it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.22it/s]\n",
            " 40% 12/30 [00:05<00:08,  2.21it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.21it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.20it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.19it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.19it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.17it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.19it/s]\n",
            " 63% 19/30 [00:08<00:05,  2.17it/s]\n",
            " 67% 20/30 [00:09<00:04,  2.18it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.16it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.16it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.15it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.15it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.14it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.13it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.13it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.14it/s]\n",
            " 97% 29/30 [00:13<00:00,  2.14it/s]\n",
            "100% 30/30 [00:13<00:00,  2.18it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7815.49 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:15<00:00,  1.98it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7815.61 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7814.88 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7814.88 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7814.88 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6790.88 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.26it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.29it/s]\n",
            " 10% 3/30 [00:01<00:11,  2.27it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.23it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.25it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.24it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.25it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.23it/s]\n",
            " 30% 9/30 [00:04<00:09,  2.23it/s]\n",
            " 33% 10/30 [00:04<00:09,  2.21it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.21it/s]\n",
            " 40% 12/30 [00:05<00:08,  2.22it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.18it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.19it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.19it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.19it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.19it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.20it/s]\n",
            " 63% 19/30 [00:08<00:05,  2.20it/s]\n",
            " 67% 20/30 [00:09<00:04,  2.17it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.19it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.19it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.17it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.17it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.16it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.15it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.15it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.15it/s]\n",
            " 97% 29/30 [00:13<00:00,  2.16it/s]\n",
            "100% 30/30 [00:13<00:00,  2.19it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7809.30 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:15<00:00,  2.00it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7809.42 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7808.69 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7808.69 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7808.69 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6784.69 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.04 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.33it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.30it/s]\n",
            " 10% 3/30 [00:01<00:11,  2.30it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.27it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.25it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.25it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.25it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.25it/s]\n",
            " 30% 9/30 [00:03<00:09,  2.24it/s]\n",
            " 33% 10/30 [00:04<00:08,  2.24it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.23it/s]\n",
            " 40% 12/30 [00:05<00:08,  2.24it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.20it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.21it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.20it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.21it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.21it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.20it/s]\n",
            " 63% 19/30 [00:08<00:04,  2.21it/s]\n",
            " 67% 20/30 [00:08<00:04,  2.19it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.19it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.16it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.17it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.17it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.15it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.15it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.15it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.14it/s]\n",
            " 97% 29/30 [00:13<00:00,  2.14it/s]\n",
            "100% 30/30 [00:13<00:00,  2.20it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7803.11 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:14<00:00,  2.01it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7803.23 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7802.50 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7802.50 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7802.50 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6778.50 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.26it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.28it/s]\n",
            " 10% 3/30 [00:01<00:11,  2.29it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.27it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.26it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.26it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.27it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.25it/s]\n",
            " 30% 9/30 [00:03<00:09,  2.25it/s]\n",
            " 33% 10/30 [00:04<00:08,  2.25it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.24it/s]\n",
            " 40% 12/30 [00:05<00:08,  2.23it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.22it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.22it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.22it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.21it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.20it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.20it/s]\n",
            " 63% 19/30 [00:08<00:05,  2.19it/s]\n",
            " 67% 20/30 [00:08<00:04,  2.19it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.18it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.17it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.17it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.16it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.15it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.14it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.13it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.13it/s]\n",
            " 97% 29/30 [00:13<00:00,  2.13it/s]\n",
            "100% 30/30 [00:13<00:00,  2.20it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7803.11 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:15<00:00,  2.00it/s]\n",
            "[Unload] Trying to free 1024.00 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7803.23 MB ... Done.\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7802.50 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7802.50 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7802.50 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6778.50 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.24it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.21it/s]\n",
            " 10% 3/30 [00:01<00:12,  2.21it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.21it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.20it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.19it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.19it/s]\n",
            " 27% 8/30 [00:03<00:10,  2.18it/s]\n",
            " 30% 9/30 [00:04<00:09,  2.18it/s]\n",
            " 33% 10/30 [00:04<00:09,  2.19it/s]\n",
            " 37% 11/30 [00:05<00:08,  2.18it/s]\n",
            " 40% 12/30 [00:05<00:08,  2.17it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.14it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.15it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.15it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.16it/s]\n",
            " 57% 17/30 [00:07<00:06,  2.16it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.15it/s]\n",
            " 63% 19/30 [00:08<00:05,  2.16it/s]\n",
            " 67% 20/30 [00:09<00:04,  2.15it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.16it/s]\n",
            " 73% 22/30 [00:10<00:03,  2.15it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.16it/s]\n",
            " 80% 24/30 [00:11<00:02,  2.16it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.16it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.16it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.16it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.15it/s]\n",
            " 97% 29/30 [00:13<00:00,  2.15it/s]\n",
            "100% 30/30 [00:13<00:00,  2.17it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7796.92 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:15<00:00,  1.99it/s]\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7796.92 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7802.50 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7802.50 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6778.50 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.26it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.24it/s]\n",
            " 10% 3/30 [00:01<00:11,  2.25it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.23it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.24it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.24it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.24it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.24it/s]\n",
            " 30% 9/30 [00:04<00:09,  2.23it/s]\n",
            " 33% 10/30 [00:04<00:08,  2.24it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.24it/s]\n",
            " 40% 12/30 [00:05<00:07,  2.25it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.23it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.24it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.23it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.23it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.23it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.24it/s]\n",
            " 63% 19/30 [00:08<00:04,  2.23it/s]\n",
            " 67% 20/30 [00:08<00:04,  2.24it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.24it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.23it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.24it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.23it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.24it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.24it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.24it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.25it/s]\n",
            " 97% 29/30 [00:12<00:00,  2.24it/s]\n",
            "100% 30/30 [00:13<00:00,  2.24it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7796.92 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:14<00:00,  2.05it/s]\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7796.92 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7796.92 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7796.92 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6772.92 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.04 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.35it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.32it/s]\n",
            " 10% 3/30 [00:01<00:11,  2.28it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.29it/s]\n",
            " 17% 5/30 [00:02<00:10,  2.28it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.27it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.24it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.24it/s]\n",
            " 30% 9/30 [00:03<00:09,  2.25it/s]\n",
            " 33% 10/30 [00:04<00:08,  2.26it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.27it/s]\n",
            " 40% 12/30 [00:05<00:07,  2.27it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.23it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.23it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.23it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.24it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.24it/s]\n",
            " 60% 18/30 [00:07<00:05,  2.25it/s]\n",
            " 63% 19/30 [00:08<00:04,  2.25it/s]\n",
            " 67% 20/30 [00:08<00:04,  2.25it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.24it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.23it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.23it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.22it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.23it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.24it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.24it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.23it/s]\n",
            " 97% 29/30 [00:12<00:00,  2.22it/s]\n",
            "100% 30/30 [00:13<00:00,  2.25it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7787.34 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:14<00:00,  2.05it/s]\n",
            "[LayerDiffuse] LayerMethod.BG_TO_BLEND\n",
            "[Unload] Trying to free 3277.32 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7787.34 MB ... Done.\n",
            "[Unload] Trying to free 2615.55 MB for cuda:0 with 0 models keep loaded ... Current free memory is 7798.38 MB ... Done.\n",
            "[Memory Management] Target: KModel, Free GPU: 7798.38 MB, Model Require: 0.00 MB, Previously Loaded: 4897.07 MB, Inference Require: 1024.00 MB, Remaining: 6774.38 MB, All loaded to GPU.\n",
            "Moving model(s) has taken 0.03 seconds\n",
            "  0% 0/30 [00:00<?, ?it/s]\n",
            "  3% 1/30 [00:00<00:12,  2.38it/s]\n",
            "  7% 2/30 [00:00<00:12,  2.26it/s]\n",
            " 10% 3/30 [00:01<00:11,  2.28it/s]\n",
            " 13% 4/30 [00:01<00:11,  2.25it/s]\n",
            " 17% 5/30 [00:02<00:11,  2.26it/s]\n",
            " 20% 6/30 [00:02<00:10,  2.25it/s]\n",
            " 23% 7/30 [00:03<00:10,  2.25it/s]\n",
            " 27% 8/30 [00:03<00:09,  2.24it/s]\n",
            " 30% 9/30 [00:03<00:09,  2.23it/s]\n",
            " 33% 10/30 [00:04<00:08,  2.24it/s]\n",
            " 37% 11/30 [00:04<00:08,  2.24it/s]\n",
            " 40% 12/30 [00:05<00:08,  2.24it/s]\n",
            " 43% 13/30 [00:05<00:07,  2.21it/s]\n",
            " 47% 14/30 [00:06<00:07,  2.22it/s]\n",
            " 50% 15/30 [00:06<00:06,  2.21it/s]\n",
            " 53% 16/30 [00:07<00:06,  2.20it/s]\n",
            " 57% 17/30 [00:07<00:05,  2.21it/s]\n",
            " 60% 18/30 [00:08<00:05,  2.19it/s]\n",
            " 63% 19/30 [00:08<00:05,  2.20it/s]\n",
            " 67% 20/30 [00:09<00:04,  2.17it/s]\n",
            " 70% 21/30 [00:09<00:04,  2.17it/s]\n",
            " 73% 22/30 [00:09<00:03,  2.16it/s]\n",
            " 77% 23/30 [00:10<00:03,  2.16it/s]\n",
            " 80% 24/30 [00:10<00:02,  2.16it/s]\n",
            " 83% 25/30 [00:11<00:02,  2.15it/s]\n",
            " 87% 26/30 [00:11<00:01,  2.14it/s]\n",
            " 90% 27/30 [00:12<00:01,  2.13it/s]\n",
            " 93% 28/30 [00:12<00:00,  2.14it/s]\n",
            " 97% 29/30 [00:13<00:00,  2.14it/s]\n",
            "100% 30/30 [00:13<00:00,  2.19it/s]\n",
            "[Unload] Trying to free 4039.62 MB for cuda:0 with 1 models keep loaded ... Current free memory is 7792.80 MB ... Done.\n",
            "\n",
            "Total progress: 100% 30/30 [00:14<00:00,  2.00it/s]\n",
            "Interrupted with signal 2 in <frame at 0x7c57f56f1900, file '/content/drive/MyDrive/stable-diffusion-webui-forge/modules_forge/main_thread.py', line 43, code loop>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_fsFs4jdrhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}