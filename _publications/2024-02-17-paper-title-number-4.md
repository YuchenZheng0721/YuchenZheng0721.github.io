---
title: "Exploring the Use of VLMs for Navigation Assistance for People with Blindness and Low Vision"
collection: publications
date: "2025-01-01"  
venue: "#"  
authors: "Yu Li, Yuchen Zheng, Giles Hamilton-Fletcher, Marco Mezzavilla, Yao Wang, Sundeep Rangan, Maurizio Porfiri, Zhou Yu, John-Ross Rizzo"
pdf: "#" 
code: "#"  
---

Yu Li, Yuchen Zheng, Giles Hamilton-Fletcher, Marco Mezzavilla, Yao Wang, Sundeep Rangan, Maurizio Porfiri, Zhou Yu, John-Ross Rizzo  

[**Paper**](../files/paper_1.pdf)  
[**Code**](#)  

## Abstract  
This paper investigates the potential of vision-language models (VLMs) to assist people with blindness and low vision (pBLV) in navigation tasks. We evaluate state-of-the-art closed-source models, including GPT-4V, GPT-4o, and Claude-3.5-Sonnet, alongside open-source models, such as Llava-v1.6-mistral and Llava-onevision-qwen, to analyze their capabilities in foundational visual skills such as counting ambient obstacles, relative spatial reasoning, and common-sense wayfinding-pertinent scene understanding.

Following this foundational evaluation, we assess each modelâ€™s performance in practical navigation scenarios, using pBLV-specific prompts designed to simulate real-world assistance tasks. Our findings reveal notable performance disparities between these models: GPT-4o consistently outperforms others across all tasks, demonstrating high accuracy and reliability, particularly in spatial reasoning and scene understanding.  

In contrast, open-source models exhibit significant shortcomings, struggling with tasks that require nuanced reasoning or adaptability in complex environments. Common challenges across models include difficulties in accurately counting objects in cluttered settings, biases in spatial reasoning, and a tendency to prioritize object details over spatial feedback, which limits their usability for pBLV in navigation tasks.  

Despite these limitations, VLMs show promise in enhancing wayfinding assistance when aligned more closely with human feedback and equipped with improved spatial reasoning capabilities. This research provides actionable insights into the strengths and limitations of current VLMs, highlighting their benefits and pitfalls in the context of pBLV assistive technologies, offering targeted guidance to developers on effectively integrating VLMs for tasks they perform well while addressing their limitations for enhanced usability.
